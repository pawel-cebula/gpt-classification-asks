{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMkhYkJ-UzDZ"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xolwvwazUy2_",
        "outputId": "97e5eed0-2798-4a71-bac2-e2a30fbd13ca"
      },
      "outputs": [],
      "source": [
        "!pip install -Uq openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBMNky9KVV7y"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtwB2WckWePp"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import openai\n",
        "import io\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYY_div4XgcM"
      },
      "source": [
        "\n",
        "## Transform uploaded file into a pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoKB1m_8aDCY",
        "outputId": "9817a5e1-c2cd-4f98-cd38-3ebd594562db"
      },
      "outputs": [],
      "source": [
        "prompt_suffix = \"\\n\\n###\\n\\n\"\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "  return prompt + prompt_suffix\n",
        "\n",
        "df_all = (\n",
        "  pd\n",
        "    .read_csv('labeled_asks.csv')\n",
        "    .rename(columns={'MSG_TEXT': 'prompt', 'Answer Type Requested': 'completion' })\n",
        "    .filter(['prompt', 'completion'], axis=1)\n",
        ")\n",
        "\n",
        "df_all['prompt'] = df_all['prompt'].apply(parse_prompt)\n",
        "df_all.to_json(\"answer_type_requested.jsonl\", orient=\"records\", lines=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare data with OpenAI data preparation tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!openai tools fine_tunes.prepare_data -f answer_type_requested.jsonl # n, Y, Y - split into training/validation samples, don't add whitespace in front of completions, we'll add it later"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZH-qb91ctSd"
      },
      "source": [
        "## Split multilabel training samples into separate rows and add completion prefix/suffix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJGwWezNcyNw",
        "outputId": "117acd0e-84b9-42f6-97c8-10f66124e88c"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_json('answer_type_requested_prepared_train.jsonl', lines=True)\n",
        "\n",
        "print(\"df_train length: \" + str(len(df_train)))\n",
        "\n",
        "df_train_processed = pd.DataFrame({\"prompt\": [], \"completion\": []})\n",
        "\n",
        "for row in df_train.itertuples():\n",
        "  labels = row.completion.split(\",\")\n",
        "  for label in labels:\n",
        "    df_train_processed.loc[len(df_train_processed)] = {'prompt': row.prompt, 'completion': label}\n",
        "\n",
        "print(\"df_train_processed length: \" + str(len(df_train_processed)))\n",
        "\n",
        "# Create label mappings to ensure all labels are single tokens\n",
        "\n",
        "unique_labels = df_train_processed['completion'].unique()\n",
        "id2label = {}\n",
        "label2id = {}\n",
        "\n",
        "for i, label in enumerate(unique_labels):\n",
        "  id2label[str(i)] = label\n",
        "\n",
        "for key, value in id2label.items():\n",
        "  label2id[value] = key\n",
        "\n",
        "print(id2label)\n",
        "print(label2id)\n",
        "\n",
        "# Parse completions in training samples\n",
        "\n",
        "completion_prefix = \" \"\n",
        "\n",
        "def parse_completion_train(completion):\n",
        "  return completion_prefix + label2id[completion]\n",
        "\n",
        "df_train_processed['completion'] = df_train_processed['completion'].apply(parse_completion_train)\n",
        "df_train_processed.to_json(\"answer_type_requested_prepared_train_processed.jsonl\", orient=\"records\", lines=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIXbpXsjccGN"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lIXHW9zcdfn",
        "outputId": "cfb8b501-f7e1-4e85-8dce-63ceb25cf150"
      },
      "outputs": [],
      "source": [
        "!openai api fine_tunes.create -t \"answer_type_requested_prepared_train_processed.jsonl\" -m davinci # not calculating classification metrics due to multilabel not being supported"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check the status of the fine-tuning job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp8YxQZi0OlE",
        "outputId": "561eddc4-4df6-49af-d0bc-bf5bcbedd118"
      },
      "outputs": [],
      "source": [
        "!openai api fine_tunes.follow -i ft-abcdef123456"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FjW5144oVgF"
      },
      "source": [
        "## Run the validation samples against fine-tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "8CVH-oRxoY0V",
        "outputId": "36ea1302-1f77-4a7a-ccef-31b614f650e2"
      },
      "outputs": [],
      "source": [
        "fine_tuned_model = 'davinci:ft-abcdef123456'\n",
        "\n",
        "df_valid = pd.read_json('answer_type_requested_prepared_valid.jsonl', lines=True)\n",
        "\n",
        "# arbitrary threshold for transforming logprobs into multilabel completion\n",
        "multilabel_threshold = 0.2\n",
        "\n",
        "def logprob_to_prob(logprob):\n",
        "  return np.exp(logprob)\n",
        "\n",
        "def calculate_accuracy(completion, labels_from_prob):\n",
        "  completion_labels = completion.split(\",\")\n",
        "  prob_labels = labels_from_prob.split(\",\")\n",
        "  accuracy = 0\n",
        "  for label in completion_labels:\n",
        "    if label in prob_labels:\n",
        "      accuracy += 1/len(completion_labels)\n",
        "  return accuracy\n",
        "\n",
        "df_valid[\"response_id\"] = \"\"\n",
        "df_valid[\"response_label\"] = \"\"\n",
        "df_valid[\"labels_from_prob\"] = \"\"\n",
        "df_valid[\"accuracy_from_prob\"] = \"\"\n",
        "\n",
        "for label in unique_labels:\n",
        "  df_valid[label] = 0\n",
        "\n",
        "for row in df_valid.itertuples():\n",
        "  res_fine_tuned = openai.Completion.create(model=fine_tuned_model, prompt=row.prompt, max_tokens=1, temperature=0, logprobs=5)\n",
        "  response_id = res_fine_tuned.choices[0].text.replace(\" \", \"\")\n",
        "  response_label = id2label[response_id]\n",
        "  df_valid.loc[row.Index, 'response_id'] = response_id\n",
        "  df_valid.loc[row.Index, 'response_label'] = response_label\n",
        "\n",
        "  labels=[]\n",
        "\n",
        "  top_logprob = res_fine_tuned.choices[0].logprobs.top_logprobs[0]\n",
        "  for key, value in top_logprob.items():\n",
        "    stripped_key = key.replace(\" \", \"\")\n",
        "    if stripped_key in id2label:\n",
        "      label = id2label[stripped_key]\n",
        "      prob = logprob_to_prob(value)\n",
        "      df_valid.loc[row.Index, label] = prob\n",
        "      if prob > multilabel_threshold:\n",
        "        labels.append(label)\n",
        "\n",
        "  # if all probabilities are below threshold, pick the completion response\n",
        "  if len(labels) == 0:\n",
        "    labels.append(response_label)\n",
        "\n",
        "  labels_from_prob = \",\".join(labels)\n",
        "  df_valid.loc[row.Index, 'labels_from_prob'] = labels_from_prob\n",
        "\n",
        "  accuracy_from_prob = calculate_accuracy(row.completion, labels_from_prob)\n",
        "  df_valid.loc[row.Index, 'accuracy_from_prob'] = accuracy_from_prob\n",
        "\n",
        "  print(row.Index)\n",
        "\n",
        "accuracy_score = df_valid['accuracy_from_prob'].mean()\n",
        "print(\"accuracy_score: \" + str(accuracy_score*100) + \"%\")\n",
        "\n",
        "df_valid.to_csv('comparison.csv')\n",
        "files.download('comparison.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
